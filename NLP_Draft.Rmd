---
title: "Prediction in Sentiment Analysis - Amazon Sample"
subtitle: "Results - Insights - Methodology"
author: "Philippe Lambot"
date: "April 14, 2020"
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r Setup, include = FALSE}
# In the YAML, I have asked a TOC (table of contents). 
# I have also chosen to produce an html_document. 

# In the opts_chunk just below, I have chosen options to avoid messages and warnings in SA_Amazon_Insights&Results.html. Messages and warnings produced by the code have already been dealt with.  
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# The next opts_chunk regulates figure layout.
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")

# The next instruction facilitates table layout in HTML.
options(knitr.table.format = "html")

# After the present chunk, there are 13 lines of code to further regulate layout:
# - the 1st block prevents bullets appearing in the TOC (Table of Contents);
# - the 2nd block determines font size in body text parts;
# - the 3rd block generates text justification.

# Output language
# Sys.setlocale("LC_ALL", "C")
```

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}
</style>

<font size="3">

<style>
body {
text-align: justify}
</style>

## <br>
## <br>
## <br>

## *********************************************************************************

## PREDICTION IN SENTIMENT ANALYSIS - AMAZON SAMPLE

## ********************************************************************************* 

<br>

## I. EXECUTIVE SUMMARY

<br>

GITHUB: https://github.com/Dev-P-L/Sentiment-Analysis

<br>

## II. FOREWORD to READERS

<br>

Dear Readers, you are most welcome to run the project on your own computer if you so wish.

This project is lodged with the GitHub repository https://github.com/Dev-P-L/Sentiment-Analysis.

New York Times blog articles from the time period September 2014-December 2014
understand the features of a blog post that make it popular
NYTimesBlogTrain.csv = the training data set. It consists of 6532 articles.
https://www.kaggle.com/c/15-071x-the-analytics-edge-competition-spring-2015 

For your convenience, the dataset has already been downloaded onto the GitHub repository wherefrom it will be automatically retrieved by the code from SA_Amazon_Code.Rmd. If you so wish, you can also easily retrieve the dataset from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences and adapt the SA_Amazon_Code.Rmd code accordingly.

You can knit SA_Amazon_Code.Rmd (please in HTML) and produce SA_Amazon_Insights&Results.html on your own computer. On my laptop, running SA_Amazon_Code.Rmd takes approximately four hours. For information, here are some characteristics of my work environment:

 - R version 3.5.1 (2018-07-02) -- "Feather Spray",
 - RStudio Version 1.1.456,
 - Windows 10.

Some packages are required in SA_Amazon_Code.Rmd. The code from SA_Amazon_Code.Rmd contains instructions to download these packages if they are not available yet. 

```{r Cleaning up workspace and downloading packages}
# Cleaning up workspace for RAM management.
invisible(if(!is.null(dev.list())) dev.off())
rm(list=ls())
cat("\014")

# Downloading packages.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(utf8)) install.packages("utf8", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm", repos = "http://cran.us.r-project.org")
if(!require(wordcloud)) install.packages("wordcloud", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# Requiring libraries.
library(tidyverse)
library(utf8)
library(lubridate)
library(tm)
library(wordcloud)
library(kableExtra)
library(caret)
```

Now, let's get in touch with data. 

<br>

## III. GETTING IN TOUCH with DATA
encoding = "UTF-8", 
<br>

```{r Dowloading data}
url <- "https://raw.githubusercontent.com/gdwangh/edxTheAnalyticsEdge/master/kaggleCompetition/NYTimesBlogTrain.csv"
ds <- read.csv(url, stringsAsFactors = FALSE)
DfNews <- ds %>% 
  mutate(PubDate = as.character(PubDate)) %>%
  mutate(Popular = as.factor(gsub(1, "Popular", gsub(0, "Unpopular", Popular))))
rm(url)
```

```{r Partitioning into training set and validation set}
set.seed(1)
ind_val <- createDataPartition(y = DfNews$Popular, 
                               times = 1, p = 1/3, list = FALSE)
ind_train <- as.integer(setdiff(1:nrow(DfNews), ind_val))
trainNews <- DfNews[ind_train, ]
valNews <- DfNews[ind_val, ]

# Keeping labels as numeric as well for further use.
y_train <- ds$Popular[ind_train]
y_val <- ds$Popular[ind_val]
rm(ds)
```

<br>

## Exploratory analysis

### A. Prevalence

<br>

```{r EDA Label breakdown by classes}
tab <- table(trainNews$Popular)
names(tab)
tab <- data.frame(matrix(tab, nrow = 1, ncol = 2, byrow = TRUE)) %>%
  `colnames<-`(names(tab)) %>%
  `rownames<-`("Training Set")
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  column_spec(3, bold = T, color = "#08457E", background = "#9bc4e2")
tab
```

<br>

Positive class is a clear minority, which justifies not relying on accuracy metric to measure predictive performance.  

<br>

```{r EDA Prevalence}
pr <- paste((round((tab[1] / nrow(trainNews)), 2) * 100), " %", sep = "" ) %>% 
  as.data.frame() %>% 
  `colnames<-`('Prevalence of Positive Class ("Popular")') %>%
  `rownames<-`("Training Set")
knitr::kable(pr, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#205030", background = "#a7e3bb")
rm(tab, pr)
```

<br>

Prevalence of positive class is 17 %. 

<br>

```{r EDA NewsDesk geom_bar}
tab <- trainNews %>% mutate(y = y_train) %>% group_by(NewsDesk) %>% 
  summarize(n = n(), perc_pop = (sum(y) / n) * 100) %>% 
  mutate(NewsDesk = reorder(NewsDesk, - perc_pop)) %>%
  select(NewsDesk, n, perc_pop) %>% as.data.frame()

graph <-  tab %>%
  ggplot(aes(NewsDesk, perc_pop)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate by News Desk") +
  xlab("News Desk") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

There is clear-cut difference between some categories: OpEd and Science are on top, much above 50 % and other categories are much lower. 

<br>

```{r EDA NewsDesk tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- trainNews %>% mutate(y = y_train) %>% group_by(NewsDesk) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 2) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(NewsDesk, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("News Desk", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

All percentages seem statistically representative except for the last four ones.

<br>

```{r EDA SectionName geom_bar}
tab <- trainNews %>% mutate(y = y_train) %>% group_by(SectionName) %>% 
  summarize(n = n(), perc_pop = (sum(y) / n) * 100) %>% 
  mutate(SectionName = reorder(SectionName, - perc_pop)) %>%
  select(SectionName, n, perc_pop) %>% as.data.frame()

graph <-  tab %>%
  ggplot(aes(SectionName, perc_pop)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate by Section Name") +
  xlab("Section Name") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 60, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

```{r EDA SectionName tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- trainNews %>% mutate(y = y_train) %>% group_by(NewsDesk) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 2) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(NewsDesk, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Section Name", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

There is clear-cut difference between some categories: Crosswords/Games, Opinion and Health are on top, above 50 % and other categories are much lower. 

<br>

```{r EDA SubsectionName geom_bar}
tab <- trainNews %>% mutate(y = y_train) %>% group_by(SubsectionName) %>% 
  summarize(n = n(), perc_pop = (sum(y) / n) * 100) %>% 
  mutate(SubsectionName = reorder(SubsectionName, - perc_pop)) %>%
  select(SubsectionName, n, perc_pop) %>% as.data.frame()

graph <-  tab %>%
  ggplot(aes(SubsectionName, perc_pop)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate by Subsection Name") +
  xlab("Subsection Name") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 60, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

```{r EDA SectionName tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- trainNews %>% mutate(y = y_train) %>% group_by(SubsectionName) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 2) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(SubsectionName, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Subsection Name", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```




<br>

.. 

<br>

```{r EDA Illustrative sample of headlines}
sample_size <- 3
set.seed(1)
seq <- sample(1:nrow(trainNews), sample_size, replace = FALSE)
sampl <- trainNews$Headline[seq]

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`("ILLUSTRATIVE SAMPLE OF HEADLINES") %>%
  `rownames<-`(paste("Training Set Row Number", seq, sep = " "))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(sample_size, seq, sampl, tab)
```

<br>

...

<br>

```{r EDA Illustrative sample of abstracts}
sample_size <- 3
set.seed(1)
seq <- sample(1:nrow(trainNews), sample_size, replace = FALSE)
sampl <- trainNews$Abstract[seq]

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`("ILLUSTRATIVE SAMPLE OF ABSTRACTS") %>%
  `rownames<-`(paste("Training Set Row Number", seq, sep = " "))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(sample_size, seq, sampl, tab)
```

<br>

.. 

<br>

```{r EDA Illustrative sample of snippets}
sample_size <- 3
set.seed(1)
seq <- sample(1:nrow(trainNews), sample_size, replace = FALSE)
sampl <- trainNews$Snippet[seq]

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`("ILLUSTRATIVE SAMPLE OF SNIPPETS") %>%
  `rownames<-`(paste("Training Set Row Number", seq, sep = " "))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(sample_size, seq, sampl, tab)
```

<br>

Consequently, should snippets be taken into account or should that predictor be discarded? How many snippets differ from abstracts because some words have been dropped? 

<br>

```{r Number of snippets that differ from abstracts because some words from snippets have been dropped?}
v <- as.logical(rep("FALSE", length(trainNews$Snippet)))
for (i in 1:length(trainNews$Snippet)) {
  v[i] <- identical(as.character(trainNews$Snippet[i]), 
                    as.character(trainNews$Abstract[i]))
}

nr_dif <- as.integer(sum(v == FALSE))
prop_dif <- paste(round((nr_dif / length(v)) * 100, 2), "%", sep = " ")
df <- data.frame(matrix(c(nr_dif, prop_dif), nrow = 2, ncol = 1),
                 stringsAsFactors = FALSE) %>%
  `rownames<-`(c("Number in Training Set", "Proportion in Training Set")) %>%
  `colnames<-`("Snippets Differing from Abstracts")

knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(v, nr_dif, prop_dif, df)
```

<br>

Snippets differing from abstracts are underrepresented. The vast majority of snippets are strictly identical to abstracts. Consequently, only one out of the two predictors will be kept. Should it be abstracts because in a tiny minority of cases they contain some additional words? Not necessarily because readers could pay more attetion to snippets. Actually, both predictors will be tried separately and the one that outperforms the other will be kept.

<br>

```{r EDA WordCount}
# Calculating quartiles.
s <- summary(trainNews$WordCount)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to include minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding to upgrade clarity in graph and table.
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

df <- df %>% mutate(values = v)

# Building up data frame with default percentages per quartile.
tab <- trainNews %>% select(WordCount) %>% 
  mutate(y = y_train) %>%
  mutate(intervals = cut(WordCount, breaks = df$values, 
                        include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_pop = ((sum(y) / n) * 100)) %>%  
  select(intervals, n, perc_pop) %>% as.data.frame()

# Drawing a geom_bar graph with default percentages. 
graph <-  tab %>%
  ggplot(aes(intervals, perc_pop)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate per Quartile of Word Count") +
  xlab("Quartile of Word Count") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br> 

The graph above is insightful: measured per quartile of WordCount, popularity rate is strictly and substantially increasing with the word count. Moreover, differences are statistically significant since each average popularity rate refers to appoximately one fourth of the training set. Consequently, Word count will be used as a predictor in Machine Learning.

<br> 

```{r EDA and data Profiling about date-time}
# Let's build up a temporary data frame encompassing chronological decomposition of date and time of publication. 
temp <- trainNews %>% 
  mutate(y = y_train) %>%
  mutate(date = as.POSIXct(PubDate)) %>%
  mutate(weekday = weekdays(date)) %>%           # names of the days of the week
  mutate(month = month(date, label = T)) %>%     # names of months
  mutate(hour = hour(date)) %>%                  # 24-hour clock
  mutate(date_week = round_date(as_datetime(date), unit = "week")) %>% 
                                                 # time series of week dates
  mutate(date_day = round_date(as_datetime(date), unit = "day")) %>%
                                                 # time series of day dates 
  as.data.frame(stringsAsFactors = FALSE)
```

<br> 

Graph with popularity average per month.

<br> 

```{r EDA Popularity average per month in graph}
graph <-  temp %>% 
  group_by(month) %>% 
  summarize(avg = mean(y)) %>%
  ggplot(aes(month, avg)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate per Month") +
  ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_blank(), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

Differences per month being Lilliputian, this predictor will not be added to the training set. Let's now have a try with days of the week. 

<br> 

```{r EDA Popularity average per weekday in graph}
# Ordering weekdays chronologically instead of alphabetically
Sys.setlocale("LC_ALL", "C")
Sys.getlocale("LC_TIME")
ordened_weekdays <- weekdays(x = as.Date(seq(7), origin = "1950-01-01"))
temp$weekday <- factor(temp$weekday, levels = ordened_weekdays)

Sys.setlocale("LC_ALL", "F")

graph <-  temp %>% 
  group_by(weekday) %>% 
  summarize(n = n(), avg = mean(y)) %>%
  ggplot(aes(weekday, avg)) + 
  geom_point(aes(size = n), color = "blue") +
  ggtitle("Popularity Rate per Weekday") +
  ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_blank(), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

There is a clear difference between weekend and other weekdays: popularity rate is substantially higher during weekend, and especially on Sundays. But are higher average popularity rates statistically representative, in other words are there enough observations for Saturday and Sunday so that their averages can be taken into account? 
<br>

```{r EDA Weekday breakdown tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- temp %>% mutate(y = y_train) %>% 
  group_by(weekday) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 4) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(weekday, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Weekday", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

The table above relativizes the predictive power of that predictor since higher average popularity rates relate to smaller numbers of observations in the training set. Nevetherless, the numbers for Saturday and Sunday remain statistically significant and this predictor will be added as well to the training set. 

<br>

```{r EDA Popularity average per hour in graph}
graph <-  temp %>% 
  group_by(hour) %>% 
  summarize(avg = mean(y)) %>%
  ggplot(aes(hour, avg)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate per Hour") +
  xlab("Hour") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

There is an upward tendency in a 24-hour clock presentation: popularity rate is rather low between 1 a.m. and 5 a.m., then rises especially from 7 p.m. onwards, culminating at almost 60 % in the 11 p.m. period. 

But are the lowest and highest average popularity rates statistically representative? Let's have a look at a breakdown of observations per hour in a 24-hour clock system. 

<br>

```{r EDA hour breakdown tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- temp %>% mutate(y = y_train) %>% 
  group_by(hour) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 3) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(hour, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Hour", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(2:(nrow(tab) - 1), bold = T, color = "#08457E", 
           background = "#9bc4e2") %>%
  row_spec((nrow(tab) - 2):nrow(tab), bold = T, color = "#333333", 
           background = "#b2b2b2")
rm(tab)
```

<br>

Observations are more concentrated in the middle of the empirical distribution. Nevertheless, even extreme rates remain statistically representative, such as the highest rate at almost 60 % or the lowest rates below 5 %, if not taken separately at least taken together. This predictor will also be included into the training set.

<br>

<br>

```{r EDA Time series of week averages}
graph <-  temp %>% 
  group_by(date_week) %>% 
  summarize(Observations = n(), avg = mean(y)) %>%
  ggplot(aes(date_week, avg)) + 
  geom_point(aes(size = Observations), color = "blue", show.legend = T) + 
  ggtitle("Time Series of Week Averages") +
  xlab("Time") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

Fluctuations are rather limited except for two points. Let's digitalize in the following table. 

<br>

```{r EDA tab of time series of week averages}
tab <- temp %>% mutate(y = y_train) %>% 
  group_by(date_week) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 3) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(date_week, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Week", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", 
           background = "#9bc4e2") 
rm(tab)
```

<br> 

Together, the highest averages correspond to almost 200 observations but these averages are not that high in comparison with other ones. Moreover, even if the highest averages were towering above the other ones, using a time series dimension would probably only make sense because here the validation relates to the same period. This predictor will not be included into the training set.  

<br>

This concludes our EDA. Let's proceed to some data profiling. Two predictors will be added to the training set: "weekday"" (days of the week) and "hour" (hour in a 24-hour clock).

```{r EDA tab of time series of week averages}
trainNews <- trainNews %>% 
  mutate(weekday = temp$weekday, hour = temp$hour) %>%
  select(- UniqueID)
```

<br>

NLP
Corpus: headlines and snippets
As a pre-attentive insight, a wordcloud will show the most frequent tokens. 

```{r Adapting opts_chunk to enlarge display width for a wordcloud}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

paste(trainNews$Headline[[i]],
                trainNews$Abstract[[i]], sep = " ")

```{r Creating corpus and bag of words}
# Combining headlines and snippets.
v <- 1:nrow(trainNews)
for (i in 1:nrow(trainNews)) {
  v[i] <- trainNews$Headline[[i]]
}

# Corpus is created on training reviews only to avoid any interference
# between training reviews and validation reviews. Otherwise, 
# tokens from validation set could (slightly) impact token selection
# when applying the sparsity threshold. 
corpus <- VCorpus(VectorSource(v)) 

# Lowercasing, removing punctuation and stopwords, stemming document.
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)

# Building up a bag of words in a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)

# Managing sparsity with sparsity threshold. 
sparse <- removeSparseTerms(dtm, 0.99)

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse <- as.data.frame(as.matrix(sparse)) 

# Making all column names R-friendly.
colnames(sentSparse) <- make.names(colnames(sentSparse))
dim(sentSparse)
# Wordcloud
set.seed(1)
wordcloud(colnames(sentSparse), colSums(sentSparse), min.freq = 1, 
          max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
```

```{r Readapting opts_chunk to go back to 60% width for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

<br>

Let's add the bag of words to the training set, the columns from the Document Term Matrix to the training set.

<br>

temp <- trainNews %>% select(- Headline, - Snippet, - Abstract, - PubDate)

```{r Adding bag of words to the training set}
temp <- trainNews %>% select(Popular, WordCount)
train <- cbind(temp, sentSparse)
```

<br>

Machine learning on train.


<br>

```{r Running svm on elementary train with headlines and snippets altogether}
fit <- train(Popular ~., data = temp, method = "rpart")
fitted <- predict(fit)
mean(fitted == train$Popular)
```






