---
title: "Popularity Prediction"
subtitle: "New York Times Blog Articles"
author: "Philippe Lambot"
date: "April 30, 2020"
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r Setup, include = FALSE}
# In the YAML above, I have asked a TOC (table of contents). 

# Avoiding messages and warnings: anyway, they have already been dealt with.  
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# Regulating figure layout.
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")

# Facilitating table layout in HTML.
options(knitr.table.format = "html")

# After the present chunk, there are 13 lines of code to further regulate layout:
# - the 1st block prevents bullets appearing in the TOC (Table of Contents);
# - the 2nd block determines font size in body text parts;
# - the 3rd block generates text justification.

# Output language
Sys.setlocale("LC_ALL", "C")
```

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}
</style>

<font size="3">

<style>
body {
text-align: justify}
</style>

## <br>
## <br>
## <br>

## ---------------------------------------------------------------------------------

## POPULARITY PREDICTION OF A SET OF NEW YORK TIMES BLOG ARTICLES               ----

## --------------------------------------------------------------------------------- 

<br>

## I. EXECUTIVE SUMMARY

<br>

a 94 % AUC rate has been reached 

GITHUB: https://github.com/Dev-P-L/Sentiment-Analysis

<br>

## II. FOREWORD to READERS

<br>

Dear Readers, you are most welcome to run the project on your own computer if you so wish.

This project is lodged with the GitHub repository https://github.com/Dev-P-L/Sentiment-Analysis.

New York Times blog articles from the time period September 2014-December 2014
understand the features of a blog post that make it popular
NYTimesBlogTrain.csv = the training data set. It consists of 6532 articles.
https://www.kaggle.com/c/15-071x-the-analytics-edge-competition-spring-2015 

For your convenience, the dataset has already been downloaded onto the GitHub repository wherefrom it will be automatically retrieved by the code from SA_Amazon_Code.Rmd. If you so wish, you can also easily retrieve the dataset from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences and adapt the SA_Amazon_Code.Rmd code accordingly.

You can knit SA_Amazon_Code.Rmd (please in HTML) and produce SA_Amazon_Insights&Results.html on your own computer. On my laptop, running SA_Amazon_Code.Rmd takes approximately four hours. For information, here are some characteristics of my work environment:

 - R version 3.5.1 (2018-07-02) -- "Feather Spray",
 - RStudio Version 1.1.456,
 - Windows 10.

Some packages are required in SA_Amazon_Code.Rmd. The code from SA_Amazon_Code.Rmd contains instructions to download these packages if they are not available yet. 

```{r Cleaning up workspace and downloading packages}
# Cleaning up workspace for RAM management.
invisible(if(!is.null(dev.list())) dev.off())
rm(list=ls())
cat("\014")

# Downloading packages.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(utf8)) install.packages("utf8", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm", repos = "http://cran.us.r-project.org")
if(!require(wordcloud)) install.packages("wordcloud", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")

# Requiring libraries.
library(tidyverse)
library(utf8)
library(lubridate)
library(tm)
library(wordcloud)
library(kableExtra)
library(caret)
library(pROC)
library(ggthemes)
```

Now, let's get in touch with data. 

<br>

## III. GETTING IN TOUCH with DATA
 
<br>

```{r Dowloading data}
url <- "https://raw.githubusercontent.com/gdwangh/edxTheAnalyticsEdge/master/kaggleCompetition/NYTimesBlogTrain.csv"
ds <- read.csv(url, encoding = "UTF-8", stringsAsFactors = FALSE)
DfNews <- ds %>% 
  mutate(NewsDesk = as.factor(NewsDesk)) %>%
  mutate(SectionName = as.factor(SectionName)) %>%
  mutate(SubsectionName = as.factor(SubsectionName)) %>%
  mutate(PubDate = as.character(PubDate)) %>%
  mutate(Popular = as.factor(gsub(1, "Popular", gsub(0, "Unpopular", Popular))))
rm(url)
```

```{r Partitioning into training set and validation set}
set.seed(1)
ind_val <- createDataPartition(y = DfNews$Popular, 
                               times = 1, p = 1/3, list = FALSE)
ind_train <- as.integer(setdiff(1:nrow(DfNews), ind_val))
trainNews <- DfNews[ind_train, ]
valNews <- DfNews[ind_val, ]

# Keeping labels as numeric as well for further use.
y_train <- ds$Popular[ind_train]
y_val <- ds$Popular[ind_val]
rm(ds)
```

<br>

## Exploratory analysis

### A. Prevalence

<br>

```{r EDA Label breakdown by classes}
tab <- table(trainNews$Popular)
names(tab)
tab <- data.frame(matrix(tab, nrow = 1, ncol = 2, byrow = TRUE)) %>%
  `colnames<-`(names(tab)) %>%
  `rownames<-`("Training Set")
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  column_spec(3, bold = T, color = "#08457E", background = "#9bc4e2")
tab
```

<br>

Positive class is a clear minority, which justifies not relying on accuracy metric to measure predictive performance.  

<br>

```{r EDA Prevalence}
pr <- paste((round((tab[1] / nrow(trainNews)), 2) * 100), " %", sep = "" ) %>% 
  as.data.frame() %>% 
  `colnames<-`('Prevalence of Positive Class ("Popular")') %>%
  `rownames<-`("Training Set")
knitr::kable(pr, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#205030", background = "#a7e3bb")
rm(tab, pr)
```

<br>

Prevalence of positive class is 17 %. 

<br>

```{r EDA NewsDesk geom_bar}
tab <- trainNews %>% mutate(y = y_train) %>% group_by(NewsDesk) %>% 
  summarize(n = n(), perc_pop = (sum(y) / n) * 100) %>% 
  mutate(NewsDesk = reorder(NewsDesk, - perc_pop)) %>%
  select(NewsDesk, n, perc_pop) %>% as.data.frame()

graph <-  tab %>%
  ggplot(aes(NewsDesk, perc_pop)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate by News Desk") +
  xlab("News Desk") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

There is clear-cut difference between some categories: OpEd and Science are on top, much above 50 % and other categories are much lower. 

<br>

```{r EDA NewsDesk tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- trainNews %>% mutate(y = y_train) %>% group_by(NewsDesk) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 2) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(NewsDesk, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("News Desk", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

All percentages seem statistically representative except for the last four ones.

<br>

```{r EDA SectionName geom_bar}
tab <- trainNews %>% mutate(y = y_train) %>% group_by(SectionName) %>% 
  summarize(n = n(), perc_pop = (sum(y) / n) * 100) %>% 
  mutate(SectionName = reorder(SectionName, - perc_pop)) %>%
  select(SectionName, n, perc_pop) %>% as.data.frame()

graph <-  tab %>%
  ggplot(aes(SectionName, perc_pop)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate by Section Name") +
  xlab("Section Name") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 60, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

```{r EDA SectionName tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- trainNews %>% mutate(y = y_train) %>% group_by(NewsDesk) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 2) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(NewsDesk, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Section Name", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

There is clear-cut difference between some categories: Crosswords/Games, Opinion and Health are on top, above 50 % and other categories are much lower. 

<br>

```{r EDA SubsectionName geom_bar}
tab <- trainNews %>% mutate(y = y_train) %>% group_by(SubsectionName) %>% 
  summarize(n = n(), perc_pop = (sum(y) / n) * 100) %>% 
  mutate(SubsectionName = reorder(SubsectionName, - perc_pop)) %>%
  select(SubsectionName, n, perc_pop) %>% as.data.frame()

graph <-  tab %>%
  ggplot(aes(SubsectionName, perc_pop)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate by Subsection Name") +
  xlab("Subsection Name") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 60, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

```{r EDA subsectionName tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- trainNews %>% mutate(y = y_train) %>% group_by(SubsectionName) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 2) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(SubsectionName, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Subsection Name", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```




<br>

.. 

<br>

```{r EDA Illustrative sample of headlines}
sample_size <- 3
set.seed(1)
seq <- sample(1:nrow(trainNews), sample_size, replace = FALSE)
sampl <- trainNews$Headline[seq]

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`("ILLUSTRATIVE SAMPLE OF HEADLINES") %>%
  `rownames<-`(paste("Training Set Row Number", seq, sep = " "))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(sample_size, seq, sampl, tab)
```

<br>

...

<br>

```{r EDA Illustrative sample of abstracts}
sample_size <- 3
set.seed(1)
seq <- sample(1:nrow(trainNews), sample_size, replace = FALSE)
sampl <- trainNews$Abstract[seq]

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`("ILLUSTRATIVE SAMPLE OF ABSTRACTS") %>%
  `rownames<-`(paste("Training Set Row Number", seq, sep = " "))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(sample_size, seq, sampl, tab)
```

<br>

.. 

<br>

```{r EDA Illustrative sample of snippets}
sample_size <- 3
set.seed(1)
seq <- sample(1:nrow(trainNews), sample_size, replace = FALSE)
sampl <- trainNews$Snippet[seq]

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`("ILLUSTRATIVE SAMPLE OF SNIPPETS") %>%
  `rownames<-`(paste("Training Set Row Number", seq, sep = " "))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(sample_size, seq, sampl, tab)
```

<br>

Consequently, should snippets be taken into account or should that predictor be discarded? How many snippets differ from abstracts because some words have been dropped? 

<br>

```{r Number of snippets that differ from abstracts because some words from snippets have been dropped?}
v <- as.logical(rep("FALSE", length(trainNews$Snippet)))
for (i in 1:length(trainNews$Snippet)) {
  v[i] <- identical(as.character(trainNews$Snippet[i]), 
                    as.character(trainNews$Abstract[i]))
}

nr_dif <- as.integer(sum(v == FALSE))
prop_dif <- paste(round((nr_dif / length(v)) * 100, 2), "%", sep = " ")
df <- data.frame(matrix(c(nr_dif, prop_dif), nrow = 2, ncol = 1),
                 stringsAsFactors = FALSE) %>%
  `rownames<-`(c("Number in Training Set", "Proportion in Training Set")) %>%
  `colnames<-`("Snippets Differing from Abstracts")

knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(v, nr_dif, prop_dif, df)
```

<br>

Snippets differing from abstracts are underrepresented. The vast majority of snippets are strictly identical to abstracts. Consequently, only one out of the two predictors will be kept. Should it be abstracts because in a tiny minority of cases they contain some additional words? Not necessarily because readers could pay more attetion to snippets. Actually, both predictors will be tried separately and the one that outperforms the other will be kept.

<br>

```{r EDA WordCount}
# Calculating quartiles.
s <- summary(trainNews$WordCount)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to include minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding to upgrade clarity in graph and table.
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

df <- df %>% mutate(values = v)

# Building up data frame with default percentages per quartile.
tab <- trainNews %>% select(WordCount) %>% 
  mutate(y = y_train) %>%
  mutate(intervals = cut(WordCount, breaks = df$values, 
                        include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_pop = ((sum(y) / n) * 100)) %>%  
  select(intervals, n, perc_pop) %>% as.data.frame()

# Drawing a geom_bar graph with default percentages. 
graph <-  tab %>%
  ggplot(aes(intervals, perc_pop)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate per Quartile of Word Count") +
  xlab("Quartile of Word Count") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br> 

The graph above is insightful: measured per quartile of WordCount, popularity rate is strictly and substantially increasing with the word count. Moreover, differences are statistically significant since each average popularity rate refers to appoximately one fourth of the training set. Consequently, Word count will be used as a predictor in Machine Learning.

<br> 

```{r EDA and data Profiling about date-time}
# Let's build up a temporary data frame encompassing chronological decomposition of date and time of publication. 
temp <- trainNews %>% 
  mutate(y = y_train) %>%
  mutate(date = as.POSIXct(PubDate)) %>%
  mutate(weekday = weekdays(date)) %>%           # names of the days of the week
  mutate(month = month(date, label = T)) %>%     # names of months
  mutate(hour = hour(date)) %>%                  # 24-hour clock
  mutate(date_week = round_date(as_datetime(date), unit = "week")) %>% 
                                                 # time series of week dates
  mutate(date_day = round_date(as_datetime(date), unit = "day")) %>%
                                                 # time series of day dates 
  as.data.frame(stringsAsFactors = FALSE)
```

<br> 

Graph with popularity average per month.

<br> 

```{r EDA Popularity average per month in graph}
graph <-  temp %>% 
  group_by(month) %>% 
  summarize(avg = mean(y)) %>%
  ggplot(aes(month, avg)) + 
  geom_bar(stat = "identity", width = 0.30, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate per Month") +
  ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_blank(), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

Differences per month being Lilliputian, this predictor will not be added to the training set. Let's now have a try with days of the week. 

```{r EDA Popularity average per weekday in graph}
# Ordering weekdays chronologically instead of alphabetically
ordened_weekdays <- weekdays(x = as.Date(seq(7), origin = "1950-01-01"))
temp$weekday <- factor(temp$weekday, levels = ordened_weekdays)

graph <-  temp %>% 
  group_by(weekday) %>% 
  summarize(n = n(), avg = mean(y)) %>%
  ggplot(aes(weekday, avg)) + 
  geom_point(aes(size = n), color = "blue") +
  ggtitle("Popularity Rate per Weekday") +
  ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_blank(), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

There is a clear difference between weekend and other weekdays: popularity rate is substantially higher during weekend, and especially on Sundays. But are higher average popularity rates statistically representative, in other words are there enough observations for Saturday and Sunday so that their averages can be taken into account? 
<br>

```{r EDA Weekday breakdown tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- temp %>% mutate(y = y_train) %>% 
  group_by(weekday) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 4) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(weekday, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Weekday", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

The table above relativizes the predictive power of that predictor since higher average popularity rates relate to smaller numbers of observations in the training set. Nevetherless, the numbers for Saturday and Sunday remain statistically significant and this predictor will be added as well to the training set. 

<br>

```{r EDA Popularity average per hour in graph}
graph <-  temp %>% 
  group_by(hour) %>% 
  summarize(avg = mean(y)) %>%
  ggplot(aes(hour, avg)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate per Hour") +
  xlab("Hour") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

There is an upward tendency in a 24-hour clock presentation: popularity rate is rather low between 1 a.m. and 5 a.m., then rises especially from 7 p.m. onwards, culminating at almost 60 % in the 11 p.m. period. 

But are the lowest and highest average popularity rates statistically representative? Let's have a look at a breakdown of observations per hour in a 24-hour clock system. 

<br>

```{r EDA hour breakdown tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- temp %>% mutate(y = y_train) %>% 
  group_by(hour) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 3) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(hour, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Hour", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(2:(nrow(tab) - 1), bold = T, color = "#08457E", 
           background = "#9bc4e2") %>%
  row_spec((nrow(tab) - 2):nrow(tab), bold = T, color = "#333333", 
           background = "#b2b2b2")
rm(tab)
```

<br>

Observations are more concentrated in the middle of the empirical distribution. Nevertheless, even extreme rates remain statistically representative, such as the highest rate at almost 60 % or the lowest rates below 5 %, if not taken separately at least taken together. This predictor will also be included into the training set.

<br>

<br>

```{r EDA Time series of week averages}
graph <-  temp %>% 
  group_by(date_week) %>% 
  summarize(Observations = n(), avg = mean(y)) %>%
  ggplot(aes(date_week, avg)) + 
  geom_point(aes(size = Observations), color = "blue", show.legend = T) + 
  ggtitle("Time Series of Week Averages") +
  xlab("Time") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

Fluctuations are rather limited except for two points. Let's digitalize in the following table. 

<br>

```{r EDA tab of time series of week averages}
tab <- temp %>% mutate(y = y_train) %>% 
  group_by(date_week) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 3) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(date_week, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Week", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", 
           background = "#9bc4e2") 
rm(tab)
```

<br> 

Together, the highest averages correspond to almost 200 observations but these averages are not that high in comparison with other ones. Moreover, even if the highest averages were towering above the other ones, using a time series dimension would probably only make sense because here the validation relates to the same period. This predictor will not be included into the training set.  

<br>

This concludes our EDA. Let's proceed to some data profiling. Two predictors will be added to the training set: "weekday"" (days of the week) and "hour" (hour in a 24-hour clock).

```{r Adjusting training set}
trainNews <- trainNews %>% 
  mutate(weekday = temp$weekday, hour = temp$hour) %>%
  select(- UniqueID)
```

<br>

NLP
Corpus: headlines and snippets
As a pre-attentive insight, a wordcloud will show the most frequent tokens. 

```{r Adapting opts_chunk to enlarge display width for a wordcloud}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```



```{r Creating corpus and bag of words}
# Combining headlines and snippets.
v <- 1:nrow(trainNews)
for (i in 1:nrow(trainNews)) {
  v[i] <- paste(trainNews$Headline[[i]],
                trainNews$Snippet[[i]], sep = " ")
}

# Corpus is created on training reviews only to avoid any interference
# between training reviews and validation reviews. Otherwise, 
# tokens from validation set could (slightly) impact token selection
# when applying the sparsity threshold. 
corpus <- VCorpus(VectorSource(v)) 

# Lowercasing, removing punctuation and stopwords, stemming document.
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)

# Building up a bag of words in a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)

# Managing sparsity with sparsity threshold. 
sparse <- removeSparseTerms(dtm, 0.975)

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse <- as.data.frame(as.matrix(sparse)) 

# Making all column names R-friendly.
colnames(sentSparse) <- make.names(colnames(sentSparse))

# Keeping it
sentSparse_train <- sentSparse

# Wordcloud
set.seed(1)
wordcloud(colnames(sentSparse), colSums(sentSparse), min.freq = 1, 
          max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
```

```{r Readapting opts_chunk to go back to 60% width for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

<br>

Let's add the bag of words to the training set, the columns from the Document Term Matrix to the training set.

<br>

```{r Adding bag of words to the training set}
base <- trainNews %>% select(- Headline, - Snippet, - Abstract, - PubDate)
train <- cbind(base, sentSparse)
```

<br>

Machine learning on train with rf and xgb.

<br>

```{r Running RF and xgb on training set}
fitControl <- trainControl(classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary,
                           returnResamp = "all",
                           savePredictions = "all")

set.seed(1)
fit_rf <- train(Popular ~ ., data = train, 
                 method = "rf", 
                 trControl = fitControl,
                 ## Specify which metric to optimize
                 metric = "ROC")

set.seed(1)
fit_xgb <- train(Popular ~ ., data = train, 
                 method = "xgbLinear", 
                 trControl = fitControl, 
                 ## Specify which metric to optimize
                 metric = "ROC")

fitted_xgb <- predict(fit_xgb)
mean(fitted_xgb == train$Popular)
confusionMatrix(fitted_xgb, train$Popular)

# caretTheme()

# trellis.par.set(caretTheme())
# densityplot(fit_xgb, pch = "|", color = "blue", lwd = 3, ylab = "Density",
         # grid = T, grid.lwd = 2, grid.col = "#DDDDDD")

```

Let's compare rf and xgb merits. First, it is only fitting that the first examined objects are the respective ROC curves and ROC AUCs. Let's start with xgb.

<br>

```{r ROC curve from xgb on the training set}
fitted_xgb_prob <- predict(fit_xgb, type = "prob")
roc_xgb <- roc(as.factor(train$Popular), fitted_xgb_prob$Popular)

graph <- plot.roc(roc_xgb, 
         lty = 1, lwd = 5,  col = "#205030", 
         identity.lty = 3, identity.lwd = 4, identity.col = "blue",
         grid = T, grid.lty = 3, grid.lwd = 2, grid.col = "#DDDDDD",
         print.auc = T, print.thres = T, 
         auc.polygon = T, auc.polygon.col="#a7e3bb", auc.polygon.lty = 2,
           auc.polygon.density = 100, auc.polygon.angle = 45,
           auc.polygon.border= "#205030", print.auc.cex= 1.75,
         main = "ROC curve from XGB Model",
         xlab = "False Negative Rate", ylab = "True Positive Rate")
graph
rm(graph)
```

<br>

The true positive rate immediately jumps to almost 1 and rapidly reaches 1 to remain there. The AUC is impressibe: 98.7 %. Now, let's turn to RF. 

<br>

```{r ROC curve from RF on the training set}
fitted_rf_prob <- predict(fit_rf, type = "prob")
roc_rf <- roc(as.factor(train$Popular), fitted_rf_prob$Popular)

graph <- plot.roc(roc_rf, 
         lty = 1, lwd = 5,  col = "magenta", 
         identity.lty = 3, identity.lwd = 4, identity.col = "blue",
         grid = T, grid.lty = 3, grid.lwd = 2, grid.col = "#DDDDDD",
         print.auc = T, print.thres = T, 
         auc.polygon = T, auc.polygon.col="mistyrose", auc.polygon.lty = 2,
           auc.polygon.density = 100, auc.polygon.angle = 45,
           auc.polygon.border= "magenta", print.auc.cex= 1.75,
         main = "ROC curve from RF Model",
         xlab = "False Negative Rate", ylab = "True Positive Rate")
graph
rm(graph)
```

<br>

This is the perfect ROC curve and AUC. The true positive rate immediately jumps to 1 and remains at that level; the AUC polygon is a perfect square 1 X 1 at the area is the maximum, i.e. 1.

RF is slightly better in ROC curve and AUC, but is it overfitting? If it is overfitting, is there more overfitting in RF than in xgb? 

There is unused information. In the train() function from caret, the default methods have been kept: parameters are tuned across three values and resampling is done on 25 bootstapped resamples. In the XGB model, there have been 675 AUC estimates: indeed, there are 27 combinations of parameter values (3 values for the parameter lambda, 3 for the parameter alpha and 3 again for the parameter nrounds) and each combination is run on 25 bootstrapped resamples. In the RF model, there have been 75 AUC esimates: 3 values for the parameter mtry, each of them being run on 25 bootstapped resamples. 

Looking at these AUC estimates might deliver insights about performance transposability to the validation set. 

First a quick summary statistic: the overall average of all AUC estimates for the two models. 

<br>

```{r Overall averages of AUCs estimates across parameters and resamples}
overall_AUC_min_xgb <- round((min(fit_xgb$resample$ROC) * 100), 2)
overall_AUC_min_rf <- round((min(fit_rf$resample$ROC) * 100), 2)

overall_AUC_avg_xgb <- round((mean(fit_xgb$resample$ROC) * 100), 2)
overall_AUC_avg_rf <- round((mean(fit_rf$resample$ROC) * 100), 2)

overall_AUC_max_xgb <- round((max(fit_xgb$resample$ROC) * 100), 2)
overall_AUC_max_rf <- round((max(fit_rf$resample$ROC) * 100), 2)

tab <- data.frame(matrix(c(overall_AUC_min_xgb, overall_AUC_min_rf,
                           overall_AUC_avg_xgb, overall_AUC_avg_rf,
                           overall_AUC_max_xgb, overall_AUC_max_rf),
                         nrow = 2, ncol = 3, byrow = FALSE)) %>%
       `colnames<-`(c("Minimum Resampled AUC", "Average Resampled AUC",
                    "Maximum Resampled AUC")) %>%
       `rownames<-`(c("eXtreme Gradient Boosting Model",
                      "Random Forest Model"))
                    
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(2, bold = T, color = "magenta", background = "mistyrose")
```

<br>


Order has been reversed: although RF delivers a perfect AUC, on resamples it is slightly outperformed by XGB. 

Would this insight be confirmed by desnity functions per combination of parameter values over all resamples, i.e. 27 combinations for XGB and 3 for RF? 

<br>

```{r Density function from xgb}
densityplot(fit_xgb, pch = "|", lwd = 3, grid = T, ylab = "Density")
```

<br>

The top of the density functions is above 92 % ROC AUC in the 27 cases. Let's turn to the RF density functions. 

<br>

```{r Density function from RF}
densityplot(fit_rf, pch = "|", lwd = 3, grid = T, ylab = "Density")
```

<br>

In two cases out of three, the top of the density function (and the main part of it as well) is below 92 %. In one case out of three, the top is just above 92 %. Two density functions having their top below 92 % might be a new insight in favor of XGB. Let's try to further discriminate. 

It would be insightful to digitalize the picture, i.e. to have numbered information instead of graphical information. Indeed, the empirical distributions above neither show average nor median. Let's calculate AUC averages and medians for all parameter value combinations (27 combinations in the case of XGB and 3 combinations in the case of RF).

<br>

```{r Averages and medians for XGB}
df <- fit_xgb$resample %>% as.data.frame(stringsAsFactors = FALSE) %>%                      group_by(lambda, alpha, nrounds) %>% 
      summarise(n = n(), avg = mean(ROC), med = median(ROC)) %>% 
      select(n, avg, med) 
df
min(df$avg)
min(df$med)
max(df$avg)
mean(df$avg)
fit_xgb$resample$ROC

# Ici valeurs optimales - Au-dessus moyennes par combinaison de paramètres
stat <- summary(resamples(list(fit_xgb, fit_rf)))
stat$statistics$ROC
```

<br>

Let's focus on two dimensions: the distributions of average AUCs per parameter value and the distributions of AUCs per resample for optimal parameter values. 

First, the distributions of average AUCs per parameter value combination. Actually, there are 27 parameter value combinations for XGB.

<br>

```{r Histogram from xgb with average AUCs per parameter value combination}
v <- fit_xgb$results$ROC
min(v)
graph <- v %>% as.data.frame() %>% 
    `colnames<-`("dist") %>%
    ggplot(aes(dist)) + 
    geom_histogram(bins = 7, color = "#08457E", fill = "#08457E") + 
    geom_vline(aes(xintercept = mean(dist)), col = "magenta", size = 2) +
    geom_vline(aes(xintercept = median(dist)), col = "yellow", 
               linetype = "dashed", size = 2) +
    ggtitle("Histogram of Average AUCs across Parameter Values for XGB") +
    theme(plot.title = element_text(vjust = -1, hjust = 0.5, 
                                    size = 13, face = "bold"),
          axis.title.x = element_blank(), axis.title.y = element_blank(), 
          axis.text.x = element_text(vjust = 2, size = 9), 
          axis.text.y = element_text(size = 9))
graph
```

<br>

Neither the average nor the median has the highest probability in this empirical density funtion from XGB. The distribution is rather concentrated. It is not centered. It is even split. But one interesting insight: the minimum value is above 92 %. 

<br>

```{r Histogram from RF}
v <- fit_rf$results$ROC

graph <- v %>% as.data.frame() %>% 
    `colnames<-`("dist") %>%
    ggplot(aes(dist)) + 
    geom_histogram(bins = 7, color = "#08457E", fill = "#08457E") + 
    geom_vline(aes(xintercept = mean(dist)), col = "magenta", size = 2) +
    geom_vline(aes(xintercept = median(dist)), col = "yellow", 
               linetype = "dashed", size = 2) +
    ggtitle("Histogram of Average AUCs across Parameter Values for RF") +
    theme(plot.title = element_text(vjust = -1, hjust = 0.5, 
                                    size = 13, face = "bold"),
          axis.title.x = element_blank(), axis.title.y = element_blank(), 
          axis.text.x = element_text(vjust = 2, size = 9), 
          axis.text.y = element_text(size = 9))
graph
```

<br>

This empirical distribution is, in this context, rather alarming: 

- the average AUC is below 91.50 %>% for RF while the XGB average is above 92.50 %>%,
- the distribution is entirely split, there is absolutely no value corresponding to the average, the biggest concentration of AUC values is on the left side below 91.2 % and so is the median. 

Let's now examine the distributions of AUC per bootstrapped resample for the optimal parameter values. 

<br>

```{r Validation set}
back_up <- valNews
valNews <- back_up %>% 
  mutate(date = as.POSIXct(PubDate)) %>%
  mutate(weekday = weekdays(date)) %>%           # names of the days of the week
  mutate(hour = hour(date)) %>%                  # 24-hour clock
  as.data.frame(stringsAsFactors = FALSE) %>%
  select(- PubDate, - Abstract, - UniqueID)

# Combining headlines and snippets.
v <- 1:nrow(valNews)
for (i in 1:nrow(valNews)) {
  v[i] <- paste(valNews$Headline[[i]],
                valNews$Snippet[[i]], sep = " ")
}

# Corpus is created on training reviews only to avoid any interference
# between training reviews and validation reviews. Otherwise, 
# tokens from validation set could (slightly) impact token selection
# when applying the sparsity threshold. 
corpus <- VCorpus(VectorSource(v)) 

# Lowercasing, removing punctuation and stopwords, stemming document.
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)

# Building up a bag of words in a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)

# Managing sparsity with sparsity threshold. 
sparse <- removeSparseTerms(dtm, 0.975)

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse <- as.data.frame(as.matrix(sparse)) 

# Making all column names R-friendly.
colnames(sentSparse) <- make.names(colnames(sentSparse))

# For machine learning, columns have to match between training set 
# and validation set: adjustments have to be made on the validation set.

# Let's keep only columns that alo exist in the training set. 
sentSparse <- sentSparse %>% as.data.frame() %>% 
               select(intersect(colnames(.), colnames(sentSparse_train)))

# Columns from train that are missing in val have to be added as null vectors.
mis <- setdiff(colnames(sentSparse_train), colnames(sentSparse))
df <- data.frame(matrix((nrow(sentSparse) * length(mis)), 
                        nrow = nrow(sentSparse), ncol = length(mis)) * 0) %>%
      `colnames<-`(mis)
buffer <- cbind(sentSparse, df) %>% as.data.frame()

val <- cbind(valNews, buffer) %>% select(- date, - Headline, - Snippet)

# Validation accuracy
pred_xgb <- predict(fit_xgb, newdata = val)
mean(pred_xgb == val$Popular)
pred_rf <- predict(fit_rf, newdata = val)
mean(pred_rf == val$Popular)
str(as.character(pred_xgb))
pred_1 <- as.numeric(gsub("Unpopular", "0", 
                          gsub("Popular", "1", as.character(pred_xgb))))
pred_2 <- as.numeric(gsub("Unpopular", "0", gsub("Popular", "1", pred_rf)))
pred <- ifelse((pred_1 + pred_2) > 0, "Popular", "Unpopular")
mean(pred == val$Popular)

# Final model -> Ensemble

df_xgb <- fit_xgb$pred %>% 
          as.data.frame(stringsAsFactors = FALSE) %>%
          filter(lambda == 0.1, alpha == 0, nrounds == 50) %>% 
          group_by(rowIndex) %>% 
          summarize(prob_popular = mean(Popular)) %>%
          select(rowIndex, prob_popular)

df_rf <- fit_rf$pred %>% 
         as.data.frame(stringsAsFactors = FALSE) %>%
         filter(mtry == 52) %>%
         group_by(rowIndex) %>% 
         summarize(prob_popular = mean(Popular)) %>%
         select(rowIndex, prob_popular)

nrow(df_xgb)
nrow(df_rf)
identical(df_xgb$rowIndex , df_rf$rowIndex)

prob_popular_ensemble <- (df_xgb$prob_popular + df_rf$prob_popular) / 2
roc(as.factor(train$Popular), prob_popular_ensemble)

# Validation AUC
pred_xgb <- predict(fit_xgb, newdata = val, type = "prob")
roc_xgb <- roc(as.factor(val$Popular), pred_xgb$Popular)
roc_xgb
auc(roc_xgb)

pred_rf <- predict(fit_rf, newdata = val, type = "prob")
roc_rf <- roc(as.factor(val$Popular), pred_rf$Popular)
roc_rf
auc(roc_rf)

hope <- data.frame(Popular = (pred_xgb$Popular + pred_rf$Popular) / 2,
                   stringsAsFactors = F) %>%
        mutate(Unpopular = 1 - Popular)
roc(as.factor(val$Popular), hope$Popular)
```



## REFERENCES

### ROC curves and AUC

https://topepo.github.io/caret/model-training-and-tuning.html#alternate-performance-metrics

https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy

Xavier Robin, Natacha Turck, Alexandre Hainard, Natalia Tiberti, Frédérique Lisacek, Jean-Charles Sanchez and Markus Müller (2011). “pROC: an open-source package for R and S+ to analyze and compare ROC curves”. BMC Bioinformatics, 12, p. 77. DOI: 10.1186/1471-2105-12-77

https://cran.r-project.org/web/packages/pROC/pROC.pdf

https://stackoverflow.com/questions/30366143/how-to-compute-roc-and-auc-under-roc-after-training-using-caret-in-r

https://www.rdocumentation.org/packages/pROC/versions/1.16.2/topics/roc

https://www.rdocumentation.org/packages/pROC/versions/1.16.2/topics/plot.roc



### Resampling and distributions

https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/trainControl

https://www.rdocumentation.org/packages/lattice/versions/0.3-1/topics/densityplot