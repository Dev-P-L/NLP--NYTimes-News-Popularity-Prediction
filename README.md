# NLP-Blog-Popularity-Prediction

<br>

## I. FOREWORD about DATA: ACKNOWLEDGEMENT and LIMITATION of USE

<br>

This research project is based on Data provided by The New York Times for a Kaggle competition. Any use of it must comply with requirements from https://developer.nytimes.com. 

<br>

## II. Files

<br>

This project is lodged with the GitHub repository https://github.com/Dev-P-L/NLP-News-Popularity-Prediction and is comprised of four files:

- README.md,
- NLP_News-Popularity-Prediction.Rmd (all code),
- NLP_News-Popularity-Prediction.html (methods, insights, results)
- and NLP_News-Popularity-Prediction.oxps (idem in other format).

<br>

## II. EXECUTIVE SUMMARY

<br>

In this project, a **94 % AUC** level has been reached in predicting news popularity on the validation set.

It has also provided **useful insights** about predictive impact from unstructured data, timing, classification and word count.

**Natural Language Processing** has been combined with **Machine Learning**. In Machine Learning, *Random Forest* has proved somewhat more performing than *eXtreme Gradient Boosting*. Working on bootstrapped resample distributions has defeated overfitting, revealed true distributions and opened up the way to an ensemble solution that has slightly outperformed both individual models.   

<br>

## III. TAGS

<br>

TAGS: popularity prediction, timing, news types, word count, headlines, snippets, AUC, ROC, Natural Language Processing, corpus, Document Term Matrix, bag of words, Machine Learning, binary classification, Random Forest, eXtreme Gradient Boosting, True Positive Rate (sensitivity), False Positive Rate, overfitting, bootstrapping iterations, resamples, density functions, distributions, ensemble solution 
