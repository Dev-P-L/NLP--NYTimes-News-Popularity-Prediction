# Access to HTML Document

https://dev-p-l.github.io/NLP--NYTimes-News-Popularity-Prediction/NLP_News_Popularity_Prediction.html

<br>
<br>

# Presentation of NLP Project on
# Prediction of NYTimes Blog Articles Popularity 

<br>

### I. ACKNOWLEDGEMENT and LIMITATION of USE

This research project is based on Data provided by The New York Times for a Kaggle competition. Any use of it must comply with requirements from https://developer.nytimes.com . 

<br>

### II. FILES

This project is lodged with the GitHub repository *https://github.com/Dev-P-L/NLP-News-Popularity-Prediction* and is comprised of four files:

- *README.md*,
- *NLP_News-Popularity-Prediction.Rmd* (all code),
- *NLP_News-Popularity-Prediction.html* (methodology, insights, results)
- and *NLP_News-Popularity-Prediction.pdf* (idem in other format).

I recommend reading methodology, insights and results in the HTML version rather than the PDF version. The HTML version offers a dynamic table of contents, external links and URLs are clickable, graphs and tables are not split over two pages and more color brings, in my perception, more readability.  

Wishing to visualize *NLP_News-Popularity-Prediction.pdf*? Very easy: just download it from GitHub.

Wishing to visualize *NLP_News-Popularity-Prediction.html*?

  * You could activate the hyperlink https://dev-p-l.github.io/NLP--NYTimes-News-Popularity-Prediction/NLP_News_Popularity_Prediction.html ?
  * Alternatively, you could open it in GitHub Desktop.
  * Alternatively again, you could knit *NLP_News-Popularity-Prediction.Rmd* or ask me by email for a copy of *NLP_News-Popularity-Prediction.html* .

<br>

### III. EXECUTIVE SUMMARY

In this project, a **94 % AUC** level has been reached in predicting blog articles popularity on the validation set.

It has also provided **useful insights** about predictive impact from unstructured data, timing, classification and word count.

**Natural Language Processing** has been combined with **Machine Learning**. In Machine Learning, *eXtreme Gradient Boosting* has proved somewhat more performing than *Random Forest*. Working on bootstrapped resample distributions has defeated overfitting, revealed true distributions and opened up the way to an ensemble solution that has slightly outperformed both individual models.   

<br>

### IV. TAGS

Popularity prediction, timing, news types, word count, headlines, snippets, AUC, ROC, Natural Language Processing, corpus, Document Term Matrix, bag of words, Machine Learning, binary classification, Random Forest, eXtreme Gradient Boosting, True Positive Rate (sensitivity), False Positive Rate, overfitting, bootstrapping iterations, resamples, density functions, distributions, ensemble solution 
