---
title: "Popularity Prediction"
subtitle: "New York Times Blog Articles"
author: "Philippe Lambot"
date: "April 30, 2020"
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r Setup, include = FALSE}
# In the YAML above, I have asked a TOC (table of contents). 

# Avoiding messages and warnings: anyway, they have already been dealt with.  
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# Regulating figure layout.
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")

# Facilitating table layout in HTML.
options(knitr.table.format = "html")

# After the present chunk, there are 13 lines of code to further regulate layout:
# - the 1st block prevents bullets appearing in the TOC (Table of Contents);
# - the 2nd block determines font size in body text parts;
# - the 3rd block generates text justification.

# Output language
Sys.setlocale("LC_ALL", "C")
```

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}
</style>

<font size="3">

<style>
body {
text-align: justify}
</style>

## <br>
## <br>
## <br>

## ---------------------------------------------------------------------------------

## POPULARITY PREDICTION OF A SET OF NEW YORK TIMES BLOG ARTICLES               ----

## --------------------------------------------------------------------------------- 

<br>

## I. EXECUTIVE SUMMARY

<br>

This project is about predicting popularity of a set of blog articles. The objective is maximizing the ROC AUC, i.e. the Area Under the Curve of the Receiver Operating Characteristic (ROC). 

A rate of **94 % AUC** has been reached using **Natural Language Processing** and an **ensemble Machine Learning solution** combining eXtreme Gradient Boosting and Random Forest. Combination has been tested with ROC curves on bootsrapped resamples. 

TAGS: AUC, ROC, NLP, corpus, document term matrix, bag of words, ML, binary classification, eXtreme Gradient Boosting, Random Forest, bootstrapping, resamples, distributions, sensitivity, specificity, ensemble solution 

<br>

GITHUB: https://github.com/Dev-P-L/Sentiment-Analysis

<br>

## II. FOREWORD to READERS

<br>

Dear Readers, you are most welcome to run the project on your own computer if you so wish.

This project is lodged with the GitHub repository https://github.com/Dev-P-L/Sentiment-Analysis.

The project is base on a dataset of New York Times blog articles from the time period September 2014-December 2014. It originates from a Kaggle competition organized by the MIT in 2015. 

For your convenience, the dataset has already been downloaded onto the GitHub repository wherefrom it will be automatically retrieved by the code from NLP_Popularity_Prediction_NYTimes.Rmd. 

On your own computer, you can knit NLP_Popularity_Prediction_NYTimes.Rmd (please in HTML) and produce NLP_Popularity_Prediction_NYTimes.html, which can in turn be converted into NLP_Popularity_Prediction_NYTimes.xps. On my laptop, running NLP_Popularity_Prediction_NYTimes.Rmd takes approximately two hours. For information, here are some characteristics of my work environment:

 - R version 3.5.1 (2018-07-02) -- "Feather Spray",
 - RStudio Version 1.1.456,
 - Windows 10.

Some packages are required in NLP_Popularity_Prediction_NYTimes.Rmd. The code from NLP_Popularity_Prediction_NYTimes.Rmd contains commands to download these packages if they are not available yet. 

<br>

```{r Cleaning up workspace and downloading packages}
# Cleaning up workspace for RAM management.
invisible(if(!is.null(dev.list())) dev.off())
rm(list=ls())
cat("\014")

# Downloading packages.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(utf8)) install.packages("utf8", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm", repos = "http://cran.us.r-project.org")
if(!require(wordcloud)) install.packages("wordcloud", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")

# Requiring libraries.
library(tidyverse)
library(utf8)
library(lubridate)
library(tm)
library(wordcloud)
library(kableExtra)
library(caret)
library(pROC)
```

Now, let's get in touch with data. 

<br>

## III. GETTING IN TOUCH with DATA
 
<br>

The dataset is plit into a validation set and a training set: the validation set contains one third of the rows from the original dataset, the training set two thirds. 

If some categories (from e.g. "SectionName") are populated in the validation set but not in the training set, nevertheless the corresponding rows remain in the validation set. This might lower AUC a little bit but in real-life prediction new data could indeed contain new categories. 

Here is a description of the training set. 

<br>

```{r Dowloading data}
url <- "https://raw.githubusercontent.com/Dev-P-L/NLP-Blog-Popularity/master/ds.csv"
ds <- read.csv(url, encoding = "UTF-8", stringsAsFactors = FALSE) %>%
      select(- 1)
DfNews <- ds %>% 
  mutate(NewsDesk = as.factor(NewsDesk)) %>%
  mutate(SectionName = as.factor(SectionName)) %>%
  mutate(SubsectionName = as.factor(SubsectionName)) %>%
  mutate(PubDate = as.character(PubDate)) %>%
  mutate(Popular = as.factor(gsub(1, "Popular", gsub(0, "Unpopular", Popular))))
rm(url)
```

```{r Partitioning into training set and validation set}
set.seed(1)
ind_val <- createDataPartition(y = DfNews$Popular, 
                               times = 1, p = 1/3, list = FALSE)
ind_train <- as.integer(setdiff(1:nrow(DfNews), ind_val))
trainNews <- DfNews[ind_train, ]
valNews <- DfNews[ind_val, ]

# Keeping labels as numerical values as well for further use.
y_train <- ds$Popular[ind_train]
y_val <- ds$Popular[ind_val]
rm(ds,DfNews)
```

```{r Showing description of training set}
str(trainNews, vec.len = 1)
```

<br>

There are ten features: 

- one feature, i.e. "Popular", contains labels ("Popular" or "Unpopular"),
- eight features are predictors,
- the last feature is an identifier. 

There are 4354 observations. 

Now, let's perform exploratory data analysis, but only on the training set since no information from the validation set should contribute to modelling. 

<br>

## IV. EXPLORATORY DATA ANALYSIS (EDA)

### A. Prevalence of Positive Class

<br>

The positive class is "Popular". 

<br>

```{r EDA Labels breakdown by classes}
df <- data.frame(matrix(table(trainNews$Popular), 
                        nrow = 1, ncol = 2, byrow = TRUE)) %>%
      `colnames<-`(names(tab)) %>%
      `rownames<-`("Training Set")

tab <- format(df, big.mark = " ")
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  column_spec(3, bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

The positive class is a clear minority, which justifies not relying on accuracy metric to measure predictive performance but on ROC AUC. Let's calculate prevalence of the positive class.   

<br>

```{r EDA Prevalence}
pr <- paste((round((df[1] / nrow(trainNews)), 2) * 100), 
            " %", sep = "" ) %>% 
      as.data.frame() %>% 
      `colnames<-`('Prevalence of Positive Class ("Popular")') %>%
      `rownames<-`("Training Set")
knitr::kable(pr, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#205030", background = "#a7e3bb")
rm(df, pr)
```

<br>

Prevalence of positive class is 17 %. 

Let's now turn to the first predictor, i.e. news desks, and draw a bar graph of it.

<br>
 
```{r EDA NewsDesk geom_bar}
tab <- trainNews %>% mutate(y = y_train) %>% group_by(NewsDesk) %>% 
  summarize(n = n(), perc_pop = (sum(y) / n) * 100) %>% 
  mutate(NewsDesk = reorder(NewsDesk, - perc_pop)) %>%
  select(NewsDesk, n, perc_pop) %>% as.data.frame()

graph <-  tab %>%
  ggplot(aes(NewsDesk, perc_pop)) + 
  geom_point(aes(size = n), color = "blue") +
  ggtitle("Popularity Rate by News Desk") +
  xlab("News Desk") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

*News desks* with average popularity rate below 20 % predominate: there are more *news desk* groups below 20 % average popularity than above and they are altogether much more populated, with the undefined group containing at least 1,200 blog articles and the *business* group at least 900. 

Nevertheless, three *news desks* stand out from the majority, and among them the *op-ed* group with an average popularity rate of almost 80 %. Moreover, the *op-ed* average popularity rate can be considered statistically representative with at least 300 blog articles. Let's digitalize information in the following table in order to get more precise information, among others about the numbers of blog articles in the *science* and *styles* categories.

<br>

```{r EDA NewsDesk tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- tab %>% 
  mutate(perc_pop = round(perc_pop, 1)) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  `colnames<-`(c("News Desk", "Occurrence", "Popularity Rate"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

The categories *op-ed*, *science* and *styles* correspond to popularity rates of 77 %, 63 % and 35 % respectively and they can be considered statistically representative in terms of number of blog articles. 

In contrast, *business*, *metro, *culture* and the undefined category, which together contain a majority of blog articles, range between 15 % and 7 % in popularity rate. 

The categories *tstyle*, *travel* and *foreign* have popularity rates of 2 % or lower.

The categories *magazine*, *national* and *sports* have a common popularity rate of 0 % but cannot be deemed to be statistically representative in terms of number of blog articles. 

In a snapshot, this is a promising insight: *news desks* show some strong and statistically representative differences and they could have substantial predictive power. 

It's time now we turned to the *section name* predictor. 

<br>

```{r EDA SectionName geom_bar}
tab <- trainNews %>% mutate(y = y_train) %>% group_by(SectionName) %>% 
  summarize(n = n(), perc_pop = (sum(y) / n) * 100) %>% 
  mutate(SectionName = reorder(SectionName, - perc_pop)) %>%
  select(SectionName, n, perc_pop) %>% as.data.frame()

graph <-  tab %>%
  ggplot(aes(SectionName, perc_pop)) + 
  geom_point(aes(size = n), color = "blue") +  
  ggtitle("Popularity Rate by Section Name") +
  xlab("Section Name") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_blank(), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 60, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

The graph above offers similarity with the graph about *news desks*: three categories have much higher popularity rates than the others. Let's have a look at a table. 

<br>

```{r EDA SectionName tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- tab %>%
  mutate(perc_pop = round(perc_pop, 1)) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  `colnames<-`(c("Section Name", "Occurrence", "Popularity Rate"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:3, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(4:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

There is a clear-cut difference between some categories: *crosswords/games*, *opinion* and *health* are on top, above 60 % and other categories are much lower.

There might be some overlapping between some categories from *news desks* and categories from *section names*: in the category *science* from *news desks*, there are 131 blog articles with an average popularity rate of 63 % and these statistics are exactly the same in the category *health* from *section names*... 

But the are solid differences as well: among *news desks*, the categories above 63 % total 489 blog articles while, among *section names*, the categories above 63 % total 624 blog articles. 

Consequently, without any further investigation, both predictors will be taken on board. 

What about *subsection names*?

<br>

```{r EDA SubsectionName geom_bar}
tab <- trainNews %>% mutate(y = y_train) %>% group_by(SubsectionName) %>% 
  summarize(n = n(), perc_pop = (sum(y) / n) * 100) %>% 
  mutate(SubsectionName = reorder(SubsectionName, - perc_pop)) %>%
  select(SubsectionName, n, perc_pop) %>% as.data.frame()

graph <-  tab %>%
  ggplot(aes(SubsectionName, perc_pop)) + 
  geom_point(aes(size = n), color = "blue") +         
  ggtitle("Popularity Rate by Subsection Name") +
  xlab("Subsection Name") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 60, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

In the graph above, there is a very high popularity rate, but how many blog articles correspond to that rate? 

<br>

```{r EDA subsectionName tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- tab %>%
  mutate(perc_pop = round(perc_pop, 1)) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  `colnames<-`(c("Subsection Name", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

Actually, the popularity rate of 91 % is not really representative from a statistical point of view. But the rate of 21 % is well representative and rather distant from lower rates, and especially from the rates of *small business*, *Asia Pacific* and *education*.

Consequently, this insight makes us accept this predictor as well. 

Up to now, three classification factors have been examined among candidate predictors. Pre-attentive insights have been garnered from graphs and precise information has been hoarded from tables. 

The three following predictors are textual information: headlines, abstracts and snippets. Let's start with a summary sample of headlines taken at random.  

<br>

```{r EDA Illustrative sample of headlines}
sample_size <- 3
set.seed(1)
seq <- sample(1:nrow(trainNews), sample_size, replace = FALSE)
sampl <- trainNews$Headline[seq]

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`("ILLUSTRATIVE SAMPLE OF HEADLINES") %>%
  `rownames<-`(paste("Training Set Row Number", seq, sep = " "))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(sample_size, seq, sampl, tab)
```

<br>

These three examples of headlines show, as expected, very tidy text. After this insight, we can dispense with some corrective measures in Natural Language Processing. 

Let's now turn to the corresponding random sample of abstracts. 

<br>

```{r EDA Illustrative sample of abstracts}
sample_size <- 3
set.seed(1)
seq <- sample(1:nrow(trainNews), sample_size, replace = FALSE)
sampl <- trainNews$Abstract[seq]

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`("ILLUSTRATIVE SAMPLE OF ABSTRACTS") %>%
  `rownames<-`(paste("Training Set Row Number", seq, sep = " "))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(sample_size, seq, sampl, tab)
```

<br>

Just as the random sample of headlines, this random sample of abstracts shows cleanness. The last piece of textual information consists of snippets.  

<br>

```{r EDA Illustrative sample of snippets}
sample_size <- 3
set.seed(1)
seq <- sample(1:nrow(trainNews), sample_size, replace = FALSE)
sampl <- trainNews$Snippet[seq]

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`("ILLUSTRATIVE SAMPLE OF SNIPPETS") %>%
  `rownames<-`(paste("Training Set Row Number", seq, sep = " "))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(sample_size, seq, sampl, tab)
```

<br>

The first and the third snippets are exactly the same as the corresponding abstracts, but, interestingly enough, the second one is a shortened version of the corresponding abstract, whose last words have been dropped. After some checking, it appears that some other abstracts have been shortened as well. How many snippets differ from abstracts because some words have been dropped at the end of them? 

<br>

```{r Number of snippets that differ from abstracts because words have been dropped}
# Building up a response vector registering the snippets that differ
v <- as.logical(rep("FALSE", length(trainNews$Snippet)))
for (i in 1:length(trainNews$Snippet)) {
  v[i] <- identical(as.character(trainNews$Snippet[i]), 
                    as.character(trainNews$Abstract[i]))
}

# Calculating the number of snippets that differ from corresponding abstracts.
nr_dif <- as.integer(sum(v == FALSE))

# Calculating the proportion of snippets that differ from abstracts. 
prop_dif <- paste(round((nr_dif / length(v)) * 100, 1), "%", sep = " ")

# Building up presentation table.
tab <- data.frame(matrix(c(nr_dif, prop_dif), nrow = 2, ncol = 1),
                  stringsAsFactors = FALSE) %>%
       `rownames<-`(c("Number in Training Set", 
                      "Proportion in Training Set")) %>%
       `colnames<-`("SNIPPETS DIFFERING FROM ABSTRACTS")

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(v, nr_dif, prop_dif, tab)
```

<br>

Snippets differing from abstracts are underrepresented. The vast majority of snippets are strictly identical to abstracts. Consequently, only one out of the two predictors will be kept. Which one will it be? Preference will be given to snippets because they might impact more on readers.

Let's now examine a numerical predictor: word count. 

<br>

```{r EDA WordCount}
# Calculating quartiles.
s <- summary(trainNews$WordCount)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to include minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding to upgrade clarity in graph and table.
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

df <- df %>% mutate(values = v)

# Building up data frame with default percentages per quartile.
tab <- trainNews %>% select(WordCount) %>% 
  mutate(y = y_train) %>%
  mutate(intervals = cut(WordCount, breaks = df$values, 
                        include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_pop = ((sum(y) / n) * 100)) %>%  
  select(intervals, n, perc_pop) %>% as.data.frame()

# Drawing a geom_bar graph with default percentages. 
graph <-  tab %>%
  ggplot(aes(intervals, perc_pop)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate per Quartile of Word Count") +
  xlab("Quartile of Word Count") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br> 

The graph above is insightful: measured per quartile of WordCount, popularity rate is strictly and substantially increasing with the word count. Moreover, differences are statistically significant since each average popularity rate refers to appoximately one fourth of the training set. Consequently, Word count will be used as a predictor in Machine Learning.

<br> 

```{r EDA and data Profiling about date-time}
# Let's build up a temporary data frame encompassing chronological decomposition of date and time of publication. 
temp <- trainNews %>% 
  mutate(y = y_train) %>%
  mutate(date = as.POSIXct(PubDate)) %>%
  mutate(weekday = weekdays(date)) %>%           # names of the days of the week
  mutate(month = month(date, label = T)) %>%     # names of months
  mutate(hour = hour(date)) %>%                  # 24-hour clock
  mutate(date_week = round_date(as_datetime(date), unit = "week")) %>% 
                                                 # time series of week dates
  mutate(date_day = round_date(as_datetime(date), unit = "day")) %>%
                                                 # time series of day dates 
  as.data.frame(stringsAsFactors = FALSE)
```

<br> 

Graph with popularity average per month.

<br> 

```{r EDA Popularity average per month in graph}
graph <-  temp %>% 
  group_by(month) %>% 
  summarize(avg = mean(y)) %>%
  ggplot(aes(month, avg)) + 
  geom_bar(stat = "identity", width = 0.30, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate per Month") +
  ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_blank(), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

Differences per month being Lilliputian, this predictor will not be added to the training set. Let's now have a try with days of the week. 

```{r EDA Popularity average per weekday in graph}
# Ordering weekdays chronologically instead of alphabetically
ordened_weekdays <- weekdays(x = as.Date(seq(7), origin = "1950-01-01"))
temp$weekday <- factor(temp$weekday, levels = ordened_weekdays)

graph <-  temp %>% 
  group_by(weekday) %>% 
  summarize(n = n(), avg = mean(y)) %>%
  ggplot(aes(weekday, avg)) + 
  geom_point(aes(size = n), color = "blue") +
  ggtitle("Popularity Rate per Weekday") +
  ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_blank(), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

There is a clear difference between weekend and other weekdays: popularity rate is substantially higher during weekend, and especially on Sundays. But are higher average popularity rates statistically representative, in other words are there enough observations for Saturday and Sunday so that their averages can be taken into account? 
<br>

```{r EDA Weekday breakdown tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- temp %>% mutate(y = y_train) %>% 
  group_by(weekday) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 4) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(weekday, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Weekday", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

The table above relativizes the predictive power of that predictor since higher average popularity rates relate to smaller numbers of observations in the training set. Nevetherless, the numbers for Saturday and Sunday remain statistically significant and this predictor will be added as well to the training set. 

<br>

```{r EDA Popularity average per hour in graph}
graph <-  temp %>% 
  group_by(hour) %>% 
  summarize(avg = mean(y)) %>%
  ggplot(aes(hour, avg)) + 
  geom_bar(stat = "identity", width = 0.40, 
           color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Popularity Rate per Hour") +
  xlab("Hour") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

There is an upward tendency in a 24-hour clock presentation: popularity rate is rather low between 1 a.m. and 5 a.m., then rises especially from 7 p.m. onwards, culminating at almost 60 % in the 11 p.m. period. 

But are the lowest and highest average popularity rates statistically representative? Let's have a look at a breakdown of observations per hour in a 24-hour clock system. 

<br>

```{r EDA hour breakdown tab}
# Drawing a geom_bar graph with popularity percentages. 
tab <- temp %>% mutate(y = y_train) %>% 
  group_by(hour) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 3) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(hour, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Hour", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(2:(nrow(tab) - 1), bold = T, color = "#08457E", 
           background = "#9bc4e2") %>%
  row_spec((nrow(tab) - 2):nrow(tab), bold = T, color = "#333333", 
           background = "#b2b2b2")
rm(tab)
```

<br>

Observations are more concentrated in the middle of the empirical distribution. Nevertheless, even extreme rates remain statistically representative, such as the highest rate at almost 60 % or the lowest rates below 5 %, if not taken separately at least taken together. This predictor will also be included into the training set.

<br>

<br>

```{r EDA Time series of week averages}
graph <-  temp %>% 
  group_by(date_week) %>% 
  summarize(Observations = n(), avg = mean(y)) %>%
  ggplot(aes(date_week, avg)) + 
  geom_point(aes(size = Observations), color = "blue", show.legend = T) + 
  ggtitle("Time Series of Week Averages") +
  xlab("Time") + ylab("Popularity Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br> 

Fluctuations are rather limited except for two points. Let's digitalize in the following table. 

<br>

```{r EDA tab of time series of week averages}
tab <- temp %>% mutate(y = y_train) %>% 
  group_by(date_week) %>% 
  summarize(n = n(), perc_pop = round((sum(y) / n), 3) * 100) %>% 
  arrange(desc(perc_pop)) %>% 
  mutate(perc_pop = paste(perc_pop, "%", sep = " "))%>%
  select(date_week, n, perc_pop) %>% as.data.frame() %>%
  `colnames<-`(c("Week", "Occurrence", "Popularity Percentage"))

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(3:nrow(tab), bold = T, color = "#08457E", 
           background = "#9bc4e2") 
rm(tab)
```

<br> 

Together, the highest averages correspond to almost 200 observations but these averages are not that high in comparison with other ones. Moreover, even if the highest averages were towering above the other ones, using a time series dimension would probably only make sense because here the validation relates to the same period. This predictor will not be included into the training set.  

<br>

This concludes our EDA. Let's proceed to some data profiling. Two predictors will be added to the training set: "weekday"" (days of the week) and "hour" (hour in a 24-hour clock).

```{r Adjusting training set}
trainNews <- trainNews %>% 
  mutate(weekday = temp$weekday, hour = temp$hour) %>%
  select(- UniqueID)
```

<br>

NLP
Corpus: headlines and snippets
As a pre-attentive insight, a wordcloud will show the most frequent tokens. 

```{r Adapting opts_chunk to enlarge display width for a wordcloud}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```



```{r Creating corpus and bag of words}
# Combining headlines and snippets.
v <- 1:nrow(trainNews)
for (i in 1:nrow(trainNews)) {
  v[i] <- paste(trainNews$Headline[[i]],
                trainNews$Snippet[[i]], sep = " ")
}

# Corpus is created on training reviews only to avoid any interference
# between training reviews and validation reviews. Otherwise, 
# tokens from validation set could (slightly) impact token selection
# when applying the sparsity threshold. 
corpus <- VCorpus(VectorSource(v)) 

# Lowercasing, removing punctuation and stopwords, stemming document.
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)

# Building up a bag of words in a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)

# Managing sparsity with sparsity threshold. 
sparse <- removeSparseTerms(dtm, 0.975)

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse <- as.data.frame(as.matrix(sparse)) 

# Making all column names R-friendly.
colnames(sentSparse) <- make.names(colnames(sentSparse))

# Keeping it
sentSparse_train <- sentSparse

# Wordcloud
set.seed(1)
wordcloud(colnames(sentSparse), colSums(sentSparse), min.freq = 1, 
          max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
```

```{r Readapting opts_chunk to go back to 60% width for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

<br>

Let's add the bag of words to the training set, the columns from the Document Term Matrix to the training set.

<br>

```{r Adding bag of words to the training set}
base <- trainNews %>% select(- Headline, - Snippet, - Abstract, - PubDate)
train <- cbind(base, sentSparse)
```

<br>

Machine learning on train with rf and xgb.

<br>

```{r Running RF and xgb on training set}
fitControl <- trainControl(classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary,
                           returnResamp = "all",
                           savePredictions = "all")

set.seed(1)
fit_rf <- train(Popular ~ ., data = train, 
                 method = "rf", 
                 trControl = fitControl,
                 ## Specify which metric to optimize
                 metric = "ROC")

set.seed(1)
fit_xgb <- train(Popular ~ ., data = train, 
                 method = "xgbLinear", 
                 trControl = fitControl, 
                 ## Specify which metric to optimize
                 metric = "ROC")

fitted_xgb <- predict(fit_xgb)
mean(fitted_xgb == train$Popular)
confusionMatrix(fitted_xgb, train$Popular)

# caretTheme()

# trellis.par.set(caretTheme())
# densityplot(fit_xgb, pch = "|", color = "blue", lwd = 3, ylab = "Density",
         # grid = T, grid.lwd = 2, grid.col = "#DDDDDD")

```

Let's compare rf and xgb merits. First, it is only fitting that the first examined objects are the respective ROC curves and ROC AUCs. Let's start with xgb.

<br>

```{r ROC curve from xgb on the training set}
fitted_xgb_prob <- predict(fit_xgb, type = "prob")
roc_xgb <- roc(as.factor(train$Popular), fitted_xgb_prob$Popular)

graph <- plot.roc(roc_xgb, 
         lty = 1, lwd = 5,  col = "#205030", 
         identity.lty = 3, identity.lwd = 4, identity.col = "blue",
         grid = T, grid.lty = 3, grid.lwd = 2, grid.col = "#DDDDDD",
         print.auc = T, print.thres = T, 
         auc.polygon = T, auc.polygon.col="#a7e3bb", auc.polygon.lty = 2,
           auc.polygon.density = 100, auc.polygon.angle = 45,
           auc.polygon.border= "#205030", print.auc.cex= 1.75,
         main = "ROC curve from XGB Model",
         xlab = "False Negative Rate", ylab = "True Positive Rate")
graph
rm(graph)
```

<br>

The true positive rate immediately jumps to almost 1 and rapidly reaches 1 to remain there. The AUC is impressibe: 98.7 %. Now, let's turn to RF. 

<br>

```{r ROC curve from RF on the training set}
fitted_rf_prob <- predict(fit_rf, type = "prob")
roc_rf <- roc(as.factor(train$Popular), fitted_rf_prob$Popular)

graph <- plot.roc(roc_rf, 
         lty = 1, lwd = 5,  col = "magenta", 
         identity.lty = 3, identity.lwd = 4, identity.col = "blue",
         grid = T, grid.lty = 3, grid.lwd = 2, grid.col = "#DDDDDD",
         print.auc = T, print.thres = T, 
         auc.polygon = T, auc.polygon.col="mistyrose", auc.polygon.lty = 2,
           auc.polygon.density = 100, auc.polygon.angle = 45,
           auc.polygon.border= "magenta", print.auc.cex= 1.75,
         main = "ROC curve from RF Model",
         xlab = "False Negative Rate", ylab = "True Positive Rate")
graph
rm(graph)
```

<br>

This is the perfect ROC curve and AUC. The true positive rate immediately jumps to 1 and remains at that level; the AUC polygon is a perfect square 1 X 1 at the area is the maximum, i.e. 1.

RF is slightly better in ROC curve and AUC, but is it overfitting? If it is overfitting, is there more overfitting in RF than in xgb? 

There is unused information. In the train() function from caret, the default methods have been kept: parameters are tuned across three values and resampling is done on 25 bootstapped resamples. In the XGB model, there have been 675 AUC estimates: indeed, there are 27 combinations of parameter values (3 values for the parameter lambda, 3 for the parameter alpha and 3 again for the parameter nrounds) and each combination is run on 25 bootstrapped resamples. In the RF model, there have been 75 AUC esimates: 3 values for the parameter mtry, each of them being run on 25 bootstapped resamples. 

Looking at these AUC estimates might deliver insights about performance transposability to the validation set. 

First a quick summary statistic: the overall average of all AUC estimates for the two models. 

<br>

```{r Overall averages of AUCs estimates across parameters and resamples}
overall_AUC_min_xgb <- round((min(fit_xgb$resample$ROC) * 100), 2)
overall_AUC_min_rf <- round((min(fit_rf$resample$ROC) * 100), 2)

overall_AUC_avg_xgb <- round((mean(fit_xgb$resample$ROC) * 100), 2)
overall_AUC_avg_rf <- round((mean(fit_rf$resample$ROC) * 100), 2)

overall_AUC_max_xgb <- round((max(fit_xgb$resample$ROC) * 100), 2)
overall_AUC_max_rf <- round((max(fit_rf$resample$ROC) * 100), 2)

tab <- data.frame(matrix(c(overall_AUC_min_xgb, overall_AUC_min_rf,
                           overall_AUC_avg_xgb, overall_AUC_avg_rf,
                           overall_AUC_max_xgb, overall_AUC_max_rf),
                         nrow = 2, ncol = 3, byrow = FALSE)) %>%
       `colnames<-`(c("Minimum Resampled AUC", "Average Resampled AUC",
                    "Maximum Resampled AUC")) %>%
       `rownames<-`(c("eXtreme Gradient Boosting Model",
                      "Random Forest Model"))
                    
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(2, bold = T, color = "magenta", background = "mistyrose")
```

<br>


Order has been reversed: although RF delivers a perfect AUC, on resamples it is slightly outperformed by XGB. 

Would this insight be confirmed by desnity functions per combination of parameter values over all resamples, i.e. 27 combinations for XGB and 3 for RF? 

<br>

```{r Density function from xgb}
densityplot(fit_xgb, pch = "|", lwd = 3, grid = T, ylab = "Density")
```

<br>

The top of the density functions is above 92 % ROC AUC in the 27 cases. Let's turn to the RF density functions. 

<br>

```{r Density function from RF}
densityplot(fit_rf, pch = "|", lwd = 3, grid = T, ylab = "Density")
```

<br>

In two cases out of three, the top of the density function (and the main part of it as well) is below 92 %. In one case out of three, the top is just above 92 %. Two density functions having their top below 92 % might be a new insight in favor of XGB. Let's try to further discriminate. 

It would be insightful to digitalize the picture, i.e. to have numbered information instead of graphical information. Indeed, the empirical distributions above neither show average nor median. Let's calculate AUC averages and medians for all parameter value combinations (27 combinations in the case of XGB and 3 combinations in the case of RF).

<br>

```{r Averages and medians for XGB}
df <- fit_xgb$resample %>% as.data.frame(stringsAsFactors = FALSE) %>%                      group_by(lambda, alpha, nrounds) %>% 
      summarise(n = n(), avg = mean(ROC), med = median(ROC)) %>% 
      select(n, avg, med) 
df
min(df$avg)
min(df$med)
max(df$avg)
mean(df$avg)
fit_xgb$resample$ROC

# Ici valeurs optimales - Au-dessus moyennes par combinaison de paramètres
stat <- summary(resamples(list(fit_xgb, fit_rf)))
stat$statistics$ROC
```

<br>

Let's focus on two dimensions: the distributions of average AUCs per parameter value and the distributions of AUCs per resample for optimal parameter values. 

First, the distributions of average AUCs per parameter value combination. Actually, there are 27 parameter value combinations for XGB.

<br>

```{r Histogram from xgb with average AUCs per parameter value combination}
v <- fit_xgb$results$ROC
min(v)
graph <- v %>% as.data.frame() %>% 
    `colnames<-`("dist") %>%
    ggplot(aes(dist)) + 
    geom_histogram(bins = 7, color = "#08457E", fill = "#08457E") + 
    geom_vline(aes(xintercept = mean(dist)), col = "magenta", size = 2) +
    geom_vline(aes(xintercept = median(dist)), col = "yellow", 
               linetype = "dashed", size = 2) +
    ggtitle("Histogram of Average AUCs across Parameter Values for XGB") +
    theme(plot.title = element_text(vjust = -1, hjust = 0.5, 
                                    size = 13, face = "bold"),
          axis.title.x = element_blank(), axis.title.y = element_blank(), 
          axis.text.x = element_text(vjust = 2, size = 9), 
          axis.text.y = element_text(size = 9))
graph
```

<br>

Neither the average nor the median has the highest probability in this empirical density funtion from XGB. The distribution is rather concentrated. It is not centered. It is even split. But one interesting insight: the minimum value is above 92 %. 

<br>

```{r Histogram from RF}
v <- fit_rf$results$ROC

graph <- v %>% as.data.frame() %>% 
    `colnames<-`("dist") %>%
    ggplot(aes(dist)) + 
    geom_histogram(bins = 7, color = "#08457E", fill = "#08457E") + 
    geom_vline(aes(xintercept = mean(dist)), col = "magenta", size = 2) +
    geom_vline(aes(xintercept = median(dist)), col = "yellow", 
               linetype = "dashed", size = 2) +
    ggtitle("Histogram of Average AUCs across Parameter Values for RF") +
    theme(plot.title = element_text(vjust = -1, hjust = 0.5, 
                                    size = 13, face = "bold"),
          axis.title.x = element_blank(), axis.title.y = element_blank(), 
          axis.text.x = element_text(vjust = 2, size = 9), 
          axis.text.y = element_text(size = 9))
graph
```

<br>

This empirical distribution is, in this context, rather alarming: 

- the average AUC is below 91.50 %>% for RF while the XGB average is above 92.50 %>%,
- the distribution is entirely split, there is absolutely no value corresponding to the average, the biggest concentration of AUC values is on the left side below 91.2 % and so is the median. 

Let's now examine the distributions of AUC per bootstrapped resample for the optimal parameter values. 

<br>

```{r Validation set}
back_up <- valNews
valNews <- back_up %>% 
  mutate(date = as.POSIXct(PubDate)) %>%
  mutate(weekday = weekdays(date)) %>%           # names of the days of the week
  mutate(hour = hour(date)) %>%                  # 24-hour clock
  as.data.frame(stringsAsFactors = FALSE) %>%
  select(- PubDate, - Abstract, - UniqueID)

# Combining headlines and snippets.
v <- 1:nrow(valNews)
for (i in 1:nrow(valNews)) {
  v[i] <- paste(valNews$Headline[[i]],
                valNews$Snippet[[i]], sep = " ")
}

# Corpus is created on training reviews only to avoid any interference
# between training reviews and validation reviews. Otherwise, 
# tokens from validation set could (slightly) impact token selection
# when applying the sparsity threshold. 
corpus <- VCorpus(VectorSource(v)) 

# Lowercasing, removing punctuation and stopwords, stemming document.
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)

# Building up a bag of words in a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)

# Managing sparsity with sparsity threshold. 
sparse <- removeSparseTerms(dtm, 0.975)

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse <- as.data.frame(as.matrix(sparse)) 

# Making all column names R-friendly.
colnames(sentSparse) <- make.names(colnames(sentSparse))

# For machine learning, columns have to match between training set 
# and validation set: adjustments have to be made on the validation set.

# Let's keep only columns that alo exist in the training set. 
sentSparse <- sentSparse %>% as.data.frame() %>% 
               select(intersect(colnames(.), colnames(sentSparse_train)))

# Columns from train that are missing in val have to be added as null vectors.
mis <- setdiff(colnames(sentSparse_train), colnames(sentSparse))
df <- data.frame(matrix((nrow(sentSparse) * length(mis)), 
                        nrow = nrow(sentSparse), ncol = length(mis)) * 0) %>%
      `colnames<-`(mis)
buffer <- cbind(sentSparse, df) %>% as.data.frame()

val <- cbind(valNews, buffer) %>% select(- date, - Headline, - Snippet)

# Validation accuracy
pred_xgb <- predict(fit_xgb, newdata = val)
mean(pred_xgb == val$Popular)
pred_rf <- predict(fit_rf, newdata = val)
mean(pred_rf == val$Popular)
str(as.character(pred_xgb))
pred_1 <- as.numeric(gsub("Unpopular", "0", 
                          gsub("Popular", "1", as.character(pred_xgb))))
pred_2 <- as.numeric(gsub("Unpopular", "0", gsub("Popular", "1", pred_rf)))
pred <- ifelse((pred_1 + pred_2) > 0, "Popular", "Unpopular")
mean(pred == val$Popular)

# Final model -> Ensemble

df_xgb <- fit_xgb$pred %>% 
          as.data.frame(stringsAsFactors = FALSE) %>%
          filter(lambda == 0.1, alpha == 0, nrounds == 50) %>% 
          group_by(rowIndex) %>% 
          summarize(prob_popular = mean(Popular)) %>%
          select(rowIndex, prob_popular)

df_rf <- fit_rf$pred %>% 
         as.data.frame(stringsAsFactors = FALSE) %>%
         filter(mtry == 52) %>%
         group_by(rowIndex) %>% 
         summarize(prob_popular = mean(Popular)) %>%
         select(rowIndex, prob_popular)

nrow(df_xgb)
nrow(df_rf)
identical(df_xgb$rowIndex , df_rf$rowIndex)

prob_popular_ensemble <- (df_xgb$prob_popular + df_rf$prob_popular) / 2
roc(as.factor(train$Popular), prob_popular_ensemble)

# Validation AUC
pred_xgb <- predict(fit_xgb, newdata = val, type = "prob")
roc_xgb <- roc(as.factor(val$Popular), pred_xgb$Popular)
roc_xgb
auc(roc_xgb)

pred_rf <- predict(fit_rf, newdata = val, type = "prob")
roc_rf <- roc(as.factor(val$Popular), pred_rf$Popular)
roc_rf
auc(roc_rf)

hope <- data.frame(Popular = (pred_xgb$Popular + pred_rf$Popular) / 2,
                   stringsAsFactors = F) %>%
        mutate(Unpopular = 1 - Popular)
roc(as.factor(val$Popular), hope$Popular)
```



## REFERENCES

### ROC curves and AUC

https://topepo.github.io/caret/model-training-and-tuning.html#alternate-performance-metrics

https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy

Xavier Robin, Natacha Turck, Alexandre Hainard, Natalia Tiberti, Frédérique Lisacek, Jean-Charles Sanchez and Markus Müller (2011). “pROC: an open-source package for R and S+ to analyze and compare ROC curves”. BMC Bioinformatics, 12, p. 77. DOI: 10.1186/1471-2105-12-77

https://cran.r-project.org/web/packages/pROC/pROC.pdf

https://stackoverflow.com/questions/30366143/how-to-compute-roc-and-auc-under-roc-after-training-using-caret-in-r

https://www.rdocumentation.org/packages/pROC/versions/1.16.2/topics/roc

https://www.rdocumentation.org/packages/pROC/versions/1.16.2/topics/plot.roc



### Resampling and distributions

https://www.rdocumentation.org/packages/caret/versions/6.0-86/topics/trainControl

https://www.rdocumentation.org/packages/lattice/versions/0.3-1/topics/densityplot